{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MadmanMarble/GLAP/blob/main/MidTermProject_original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peZYizUVGBsK"
      },
      "source": [
        "#Mid Term Project\n",
        "##Description of the mid term project: this project implements linear regression from scratch with stochastic gradient descent, without using a library function except of course the necessary functions for array processing and autograd. The given code implements linear regression with the MSE loss function and the assignment is to experiment with other loss functions.\n",
        "***Notes***: The linear regression code for this notebook is based on ***3.2. Linear Regression Implementation from Scratch*** of the [Dive into Deep Learning Book](https://d2l.ai/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmPAQhgpH61S"
      },
      "source": [
        "!pip install -U d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppr2eMwXHiL9"
      },
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S7l1IXoH-f3"
      },
      "source": [
        "def synthetic_data(w, b, num_examples):\n",
        "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
        "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
        "    y = torch.matmul(X, w) + b\n",
        "    y += torch.normal(0, 0.01, y.shape)\n",
        "    return X, y.reshape((-1, 1))\n",
        "\n",
        "true_w = torch.tensor([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = synthetic_data(true_w, true_b, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZkbBvxHIOi6"
      },
      "source": [
        "print('features:', features[0], '\\nlabel:', labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyqZsF8iITV1"
      },
      "source": [
        "d2l.set_figsize()\n",
        "# The semicolon is for displaying the plot only\n",
        "d2l.plt.scatter(features[:, 1].detach().numpy(),\n",
        "                labels.detach().numpy(), 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3az2hO5lIYq1"
      },
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # The examples are read at random, in no particular order\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        batch_indices = torch.tensor(indices[i:min(i +\n",
        "                                                   batch_size, num_examples)])\n",
        "        yield features[batch_indices], labels[batch_indices]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSgylI2hIiV-"
      },
      "source": [
        "batch_size = 10\n",
        "\n",
        "for X, y in data_iter(batch_size, features, labels):\n",
        "    print(X, '\\n', y)\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUQwlJe0InJ6"
      },
      "source": [
        "w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5loPBe7lI6VZ"
      },
      "source": [
        "def linreg(X, w, b):\n",
        "    \"\"\"The linear regression model.\"\"\"\n",
        "    return torch.matmul(X, w) + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1py56YuI_is"
      },
      "source": [
        "def squared_loss(y_hat, y):\n",
        "    \"\"\"Squared loss.\"\"\"\n",
        "    return (y_hat - y.reshape(y_hat.shape))**2 / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OegPZxgLJGQH"
      },
      "source": [
        "def sgd(params, lr, batch_size):\n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad / batch_size\n",
        "            param.grad.zero_()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShiJFNc9LBi5"
      },
      "source": [
        "lr = 0.03\n",
        "num_epochs = 3\n",
        "net = linreg\n",
        "loss = squared_loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter(batch_size, features, labels):\n",
        "        l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`\n",
        "        # Compute gradient on `l` with respect to [`w`, `b`]\n",
        "        l.sum().backward()\n",
        "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
        "    with torch.no_grad():\n",
        "        train_l = loss(net(features, w, b), labels)\n",
        "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXNgxNm0LSQl"
      },
      "source": [
        "print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')\n",
        "print(f'error in estimating b: {true_b - b}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMGkGcdAyYO2"
      },
      "source": [
        "#Assigned tasks:\n",
        "##The MSE loss function is not the only loss function for linear regression. To be robust to outliers in the training data, the Huber loss and the Log-Cosh are used. Your task is to implement both loss functions in addition to the given MSE loss function (named squared_loss) of this notebook, and to compare their performance on data with outliers. The specific tasks are as follows -\n",
        "1.   Add two more code cells - one that defines the Huber loss function, and another with defines the Log-Cosh. This [blog post](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0) describes these loss functions and provides code which is written in numpy so it will need to changed to compatible with PyTorch.\n",
        "2.   Inject outliers in the data generated by the synthetic data generator of this notebook. You could add random noise to the existing data objects, i.e. changing a given percentage of the data entries to random values within the data range. Inclusion of these outliers will let you compare the benefits of these two loss functions against the MSE.\n",
        "3.   Create a text cell to summarize your experimental observations as follows: (1) note how many outliers you added (2) compare the error in estimating w and b between MSE, Huber loss, and Log-Cosh loss.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLducvFuWcLT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}